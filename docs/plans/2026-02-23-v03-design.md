# v0.3 Design — Syntrix (SignalOps)

> **Date:** 2026-02-23
> **Status:** Approved
> **Milestone:** v0.3 (60-day)
> **Execution:** 4 parallel terminals

---

## Overview

v0.3 adds seven major features to Syntrix, transforming it from a CLI-only tool into a full
web-enabled platform with fine-tuned model support, multi-platform intelligence, and an
extensible scoring system.

## Features

| # | Feature | Terminal | Priority |
|---|---------|----------|----------|
| 1 | Web Dashboard (FastAPI + React SPA) | T1 | High |
| 2 | Fine-Tuned Model Deployment | T2 | High |
| 3 | A/B Testing (canary routing) | T2 | High |
| 4 | DPO Preference Data Collection | T2 | Medium |
| 5 | Batch Processing Mode | T3 | Medium |
| 6 | Plugin/Scoring System | T3 | Medium |
| 7 | LinkedIn Adapter (read-only stub) | T4 | Low |

## Architecture Changes

### New Packages

```
src/signalops/api/           # FastAPI backend (T1)
    __init__.py
    app.py                   # FastAPI application factory
    auth.py                  # API key / JWT auth
    deps.py                  # Dependency injection (DB session, config)
    routes/
        __init__.py
        projects.py          # /api/projects
        leads.py             # /api/leads
        queue.py             # /api/queue (drafts approval)
        stats.py             # /api/stats
        analytics.py         # /api/analytics
        experiments.py       # /api/experiments (A/B tests)
    schemas.py               # Pydantic response models
    websocket.py             # Real-time pipeline progress

dashboard/                   # React SPA (T1)
    package.json
    vite.config.ts
    src/
        App.tsx
        pages/
            Dashboard.tsx
            Leads.tsx
            Queue.tsx
            Analytics.tsx
            Experiments.tsx
            Settings.tsx
        components/
            ... (UI components)
        hooks/
            ... (data fetching, websocket)
        lib/
            api.ts           # API client

src/signalops/models/
    ab_test.py               # ABTestJudge (T2)
    finetuned.py             # FineTunedJudge (T2)

src/signalops/training/
    dpo.py                   # DPO preference pair collector (T2)

src/signalops/pipeline/
    batch.py                 # BatchCollector (T3)

src/signalops/scoring/       # New package (T3)
    __init__.py
    base.py                  # ScoringPlugin ABC
    weighted.py              # Current scorer extracted
    keyword_boost.py         # Keyword boost plugin
    account_age.py           # Account age plugin
    engine.py                # Plugin orchestrator (includes config-driven rules)

src/signalops/connectors/
    linkedin.py              # LinkedIn connector stub (T4)
    factory.py               # ConnectorFactory (T4)
```

### Database Schema Changes

**New tables:**

```sql
-- Model registry (T2)
CREATE TABLE model_registry (
    id                INTEGER PRIMARY KEY AUTOINCREMENT,
    model_id          VARCHAR(256) NOT NULL UNIQUE,
    provider          VARCHAR(32) NOT NULL,       -- "openai", "anthropic"
    model_type        VARCHAR(32) NOT NULL,       -- "judge", "drafter"
    display_name      VARCHAR(256),               -- Human-friendly name
    base_model        VARCHAR(128),               -- Original model fine-tuned from
    training_file     VARCHAR(512),               -- Path to training JSONL used
    training_examples INTEGER,                    -- Number of training examples
    version           VARCHAR(64),
    deployed_at       DATETIME DEFAULT CURRENT_TIMESTAMP,
    is_active         BOOLEAN DEFAULT TRUE,
    metrics           JSON,                       -- {"precision": 0.92, "recall": 0.85, ...}
    metadata          JSON
);

-- A/B test experiments (T2)
CREATE TABLE ab_experiments (
    id              INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_id   VARCHAR(64) NOT NULL UNIQUE,
    project_id      VARCHAR(64) REFERENCES projects(id),
    primary_model   VARCHAR(128) NOT NULL,
    canary_model    VARCHAR(128) NOT NULL,
    canary_pct      FLOAT NOT NULL DEFAULT 0.1,
    status          VARCHAR(16) NOT NULL DEFAULT 'active',  -- active, paused, completed
    started_at      DATETIME DEFAULT CURRENT_TIMESTAMP,
    ended_at        DATETIME,
    metadata        JSON
);

-- A/B test results per judgment (T2)
CREATE TABLE ab_results (
    id              INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_id   VARCHAR(64) REFERENCES ab_experiments(experiment_id),
    judgment_id     INTEGER REFERENCES judgments(id),
    model_used      VARCHAR(128) NOT NULL,
    latency_ms      FLOAT,
    created_at      DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- DPO preference pairs (T2)
CREATE TABLE preference_pairs (
    id              INTEGER PRIMARY KEY AUTOINCREMENT,
    draft_id        INTEGER REFERENCES drafts(id),
    project_id      VARCHAR(64) REFERENCES projects(id),
    prompt          TEXT NOT NULL,
    chosen_text     TEXT NOT NULL,
    rejected_text   TEXT NOT NULL,
    source          VARCHAR(32) NOT NULL,   -- "edit", "reject", "manual"
    created_at      DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Scoring plugin results (T3)
CREATE TABLE scoring_plugin_results (
    id                  INTEGER PRIMARY KEY AUTOINCREMENT,
    score_id            INTEGER REFERENCES scores(id),
    plugin_name         VARCHAR(128) NOT NULL,
    plugin_version      VARCHAR(64),
    plugin_score        FLOAT NOT NULL,
    details             JSON,
    created_at          DATETIME DEFAULT CURRENT_TIMESTAMP
);
```

**Modified tables:**

```sql
-- scores: add scoring_plugins column
ALTER TABLE scores ADD COLUMN scoring_plugins JSON;  -- List of plugins used

-- judgments: add experiment_id column
ALTER TABLE judgments ADD COLUMN experiment_id VARCHAR(64);
```

### Config Schema Extensions

```yaml
# project.yaml additions

# Scoring plugins (T3)
scoring:
  # ... existing weights ...
  custom_rules:
    - name: "keyword_boost"
      condition: "text contains 'looking for'"
      boost: 10
    - name: "verified_bonus"
      condition: "author_verified == true"
      boost: 15
  plugins:
    - "signalops.scoring.keyword_boost"
    - "signalops.scoring.account_age"

# Batch processing (T3)
batch:
  enabled: false
  concurrency: 3
  retry_failed: true

# Platform connectors (T4)
platforms:
  x:
    enabled: true
    # ... existing X config ...
  linkedin:
    enabled: false
    # ... future LinkedIn config ...

# A/B testing (T2)
experiments:
  enabled: false
  default_canary_pct: 0.1
```

### New Dependencies

```toml
# pyproject.toml additions
dependencies = [
    # ... existing ...
    "fastapi>=0.115",
    "uvicorn[standard]>=0.32",
    "python-jose[cryptography]>=3.3",
    "python-multipart>=0.0.12",
    "websockets>=13.0",
    "litellm>=1.55",              # Replaces hand-rolled llm_gateway + providers (T2)
    "langfuse>=2.50",             # LLM observability and A/B test tracing (T2)
]

[project.optional-dependencies]
argilla = ["argilla>=2.0"]        # Optional: DPO data annotation UI export (T2)
```

### Key Integrations (v0.3)

| Integration | Purpose | Terminal |
|-------------|---------|----------|
| **[LiteLLM](https://github.com/BerriAI/litellm)** (16k stars) | Replaces custom LLM gateway — unified API, fallbacks, cost tracking, 100+ providers | T2 (Phase 0) |
| **[Langfuse](https://github.com/langfuse/langfuse)** (19k stars) | LLM observability — trace every call, power A/B test analytics, cost dashboards | T2 (Phase 2.5) |
| **[Argilla](https://github.com/argilla-io/argilla)** (4k stars) | Optional annotation UI for reviewing DPO preference pairs before fine-tuning | T2 (Phase 3) |

## Terminal Coordination

### Phase 1: Foundation (Days 1-5)
All 4 terminals work in parallel on isolated packages. No shared file conflicts.

### Phase 2: Feature Build (Days 6-12)
T1-T4 continue building within their domains. T2 and T3 touch `schema.py` —
coordinate by having T3 merge first, T2 rebases (matching the Phase 3 merge
order: T4 → T3 → T2 → T1).

### Phase 3: Integration (Days 13-16)
Sequential merges: T4 → T3 → T2 → T1 (reverse dependency order).
T1 merges last because it depends on all other features being in the codebase
to expose them via API endpoints.

### Phase 3.5: Alembic Migrations (Day 16, after all merges)
Generate a **single shared Alembic migration** covering all schema changes from
all 4 terminals. This avoids conflicting migration chains:
- 5 new tables: `model_registry`, `ab_experiments`, `ab_results`, `preference_pairs`, `scoring_plugin_results`
- 2 altered tables: `scores` (add `scoring_plugins` JSON column), `judgments` (add `experiment_id` column)
- Run: `alembic revision --autogenerate -m "v0.3 schema additions"`
- Verify migration applies cleanly to a fresh v0.2 database

### Phase 4: Polish (Days 17-20)
All terminals: cross-feature integration tests, documentation, CI updates.

## Success Criteria

1. Web dashboard loads, displays leads, and allows approve/edit/reject
2. Fine-tuned model can be swapped in via config change
3. A/B test runs, records results, and shows statistical comparison
4. DPO preference pairs are auto-generated from draft edits/rejections
5. Batch collection runs 3 queries concurrently, respecting rate limits
6. Custom scoring rules apply from project.yaml
7. LinkedIn connector stub passes interface tests
8. All existing tests continue to pass
9. CI pipeline green (ruff, mypy --strict, pytest)
